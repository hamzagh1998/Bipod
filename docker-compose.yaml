services:
  ollama:
    image: ollama/ollama:latest
    container_name: bipod_ollama
    restart: unless-stopped
    volumes:
      - ./data/ollama:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  imagine:
    build:
      context: ./imagine
      dockerfile: ../docker/Dockerfile.imagine
    container_name: bipod_imagine
    restart: unless-stopped
    ports:
      - "3333:3333"
    environment:
      - HF_HOME=/app/models
      - OFFLINE_MODE=${OFFLINE_MODE:-true}
    volumes:
      - ./imagine:/app
      - ./data/models:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  bipod-app:
    build:
      context: .
      dockerfile: docker/Dockerfile.app
    container_name: bipod_brain
    restart: unless-stopped
    depends_on:
      - ollama
      - imagine
    ports:
      - "4444:4444"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - ENABLE_IMAGINE=true
      - IMAGINE_API_URL=http://imagine:3333
      - PYTHON_JIT=${PYTHON_JIT:-on}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    devices:
      - "/dev/snd:/dev/snd"
      - "/dev/video0:/dev/video0"
    group_add:
      - audio
      - video
    volumes:
      - ./app:/app/app
      - ./frontend:/app/frontend
      - ./data:/app/data
      - /:/host:rw

networks:
  default:
    name: bipod-network